Project # 3	EBay Learning Network Store, eBay IPC, Chennai                        March 2012 – Jun 2013          
Designation	Developer
Client	EBay Inc.
Description	 EBay India is an e-commerce web application meant for online shoppers in India. Recommendation system is built for this site to drive more conversions
                 These are some of the features of a typical recommendation system
                 -->Suggest items to users which they are more likely to be interested in based on the user's preference profile .
                 -->Complimentary purchase feature recommends the complimentary items which most users frequently buy together.
                 -->Similar Product feature gives content based recommendation to the user.
                 -->Also take care to prevent from recommending compeltely irrelevant items to the user.

                User preference data is stored in personalization system this data is used for webanalytics purposes is also souce for recommendation engine.Adapting the input to recommender algorithms is usually quite a problem-specific process. Discovering the best recommender engine to apply to the input data is likewise specific to each context. It inevitably involves hands-on exploration, experimentation, and evaluation on real problem data. And repeat it many times.

                Hadoop is a popular distributed computing framework that includes two components of interest: a distributed file system, HDFS, and an implementation of the MapReduce paradigm. A simple recommender algorithm will take five MapReduce stages. Mahout machine learning library avaible for use with hadoop is used to build both user-based and item-based recommendors. Models are evaluated  using the available recommendors with the library such as GenericUserBasedRecommender, GenericItemBasedRecommender, SlopeOneRecommender, SVDRecommender and TreeClusteringRecommender. Different similarity metrics such as LogLikelihood,  Tanimoto, Pearson and Euclidean are  tested to build the right model for the problem. A user-based recommender using a Euclidean distance-based similarity metric and a nearest-2 neighborhood definition gave promising results with accuracy. Recommendor's results are stored in HBase and served to the clent via HTTP and SOAP



Responsibilities
-->User activity with browser is tracked by webanalytics tools and user profile is created in personalization system.
-->Developed sqoop scripts to move this preference data to HDFS
-->Developed the Mapreduce jobs which are used to clean the data. In total MapReduce jobs are run in five stages to extract the recommendations.
-->Used Mahout library to build and evaluate models using various available algorithms.
-->Strived to achieve performance benchmarks. Recommendation engine performance found to be  acceptable (about 500 ms per recommendation)
-->Developed Recommendation server to serve REST and SOAP clients.

Technologies Used :	Hadoop, Mahout, Sqoop, HBase Tomcat, REST Webservices


------------
Adapting the input to recommender algorithms is usually quite a problem-specific process. Discovering the best recommender engine to apply to the input data is likewise specific to each context. It inevitably involves hands-on exploration, experimentation, and evaluation on real problem data.

You will try an approach, collect data, understand the results, and repeat many times.

Both user-based and item-based recommendors are used 

Different Algorithms : GenericUserBasedRecommender, GenericItemBasedRecommender, SlopeOneRecommender, SVDRecommender, TreeClusteringRecommender

Similarity metrics : LogLikelihood,  Tanimoto, Pearson, Euclidean

Finally a user-based recommender using a Euclidean distance-based similarity metric and a nearest-2 neighborhood definition is chosen.

IDRescorer interface, a practical tool that can be used to modify results in ways specific to a problem domain.That is excluding recommendations
 based known data .
 
tested performance and found it acceptable (about 500 ms per recommendation),
we constructed a deployable version of the recommender engine and automatically
created a web-enabled application around it using Mahout. We briefly
examined how to deploy and access this component via HTTP and SOAP.


--------GenericUserBasedRecommender
Key Parameters :
 • User similarity metric
 • Neighborhood definition and size
Key features :
 • “Conventional” implementation
 • Fast when number of users is relatively smaller

---------GenericItemBasedRecommender

Key Parameters
• Item similarity metric
Key features
• Fast when number of items is relatively smaller
• Useful when an external notion of item similarity is available
---------SlopeOneRecommender
Key Parameters
• Diff storage strategy
Key features
• Recommendations and updates fast at runtime
• Requires large precomputation
• Suitable when number of items is relatively small
---------SVDRecommender
Key Parameters
• Number of features
Key features
• Good results
• Requires large precomputation
---------KnnItemBasedRecommender
Key Parameters
• Number of means (“k”)
• Item similarity metric
. Neighborhood size
Key features
• Good when number of items is relatively smaller

---------TreeClusteringRecommender
Key Parameters
• Number of clusters
• Cluster similarity definition
• User similarity metric
Key features
• Recommendations are fast at runtime
• Requires large precomputation
• Good when number of users is relatively smaller



Deploying a non-distributed recommender engine based on this data could prove difficult. A solution lies in using many small machines, not one big one, for all the reasons that using one big machine is undesirable. To be clear, distributing a computation doesn’t make it more efficient. On the contrary, it usually makes it require significantly more resources.It should be noted that such large, distributed computations are necessarily performed offline, not in real time in response to user requests. 


Hadoop, as noted before, is a popular distributed computing framework that includes two components of interest: a distributed file system, HDFS, and an implementation of the MapReduce paradigm.

simple recommender algorithm will take five MapReduce stages
The first MapReduce will construct user vectors:  “userID: itemID1 itemID2 itemID3 …”.   =>  “userID,itemID,preference”
                                                                                             98955 / [590:1.0, 22:1.0, 9059:1.0]
The next phase of the computation is another MapReduce that uses the output of the first MapReduce to compute co-occurrences.

It’s now possible to use MapReduce to multiply the user vectors computed in step 1,
and the co-occurrence matrix from step 2, to produce a recommendation vector from
which the algorithm can derive recommendations


Translating to MapReduce:
matrix multiplication by partial products

Translating to MapReduce: making recommendations
The output ultimately exists as one or more files stored on an HDFS instance, as a
compressed text file. The lines in the text file are of this form:
3 [103:24.5,102:18.5,106:16.5]


Each user ID is followed by a comma-delimited list of item IDs that have been recommended
(followed by a colon and the corresponding entry in the recommendation
vector, for what it’s worth). This output can be retrieved from HDFS, parsed, and used
in an application. Note that the output from Mahout will be compressed using gzip, to
save space.